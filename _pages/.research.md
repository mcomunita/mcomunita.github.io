---
permalink: /research/
layout: single
title: "Research"
author_profile: true
---

The objective of my research is to build human-centered machine listening and audio processing systems that enable new interactions for observing the acoustic world and expressing oneself through sound. 
I aim to use machine learning as a tool to augment human skills/intelligence rather than as a tool to fully automate processes. 
I carry out this research in the fields of human-computer interaction (HCI) and machine learning, and I apply it to the content domain of audio, a rich application space due to the inherent subjectivity, ambiguity, and context-dependence of auditory perception. 
To carry out my research, I develop new methods, conduct human subject experiments at scale with crowdsourcing, build working systems, and study users interacting with these systems to gain insights.

## Sound Event Detection
![sonyc](/assets/images/sonyc.png){: .align-left height="250px" width="250px"}
Sound event detection (SED) aims to detect and describe the events in an acoustic scene given a continuous acoustic signal. 
It has the potential to enable powerful applications in diverse domains such as bioacoustic monitoring, urban noise monitoring, music transcription, electric vehicle sensing, assistive technologies, and more.
I research many aspects of sound event detection pipeline, including best practices for audio annotation, audio representation learning for SED, model compression, and interactive SED systems for making sense of large scale audio collections.
Most of this research has been conducted in the context of the [Sounds of New York City (SONYC) project](http://wp.nyu.edu/sonyc), a large NSF-funded project to monitor, analyze, and mitigate urban noise pollution.

### Selected publications ([see all](/publications/sound-event-detection))
- [TriCycle: Audio Representation Learning from Sensor Network Data Using Self-Supervision (*WASPAA*)](/publications/2019-10-01-cartwright2019tricycle)
- [SONYC Urban Sound Tagging (SONYC-UST): A Multilabel Dataset from an Urban Acoustic Sensor Network (*DCASE*)](/publications/2019-10-01-cartwright2019sonycust)
- [Crowdsourcing Multi-label Audio Annotation Tasks with Citizen Scientists (*CHI*)](/publications/2019-04-01-cartwright2019crowdsourcing)
- [Increasing Drum Transcription Vocabulary Using Data Synthesis (*DAFx*)](/publications/2018-09-01-cartwright2018increasing)
- [Seeing Sound: Investigating the Effects of Visualizations and Complexity on Crowdsourced Audio Annotations (*CSCW*)](/publications/2017-11-01-cartwright2017seeing)

## Natural Audio Production Interfaces
![synthassist](/assets/images/synthassist.png){: .align-left height="250px" width="250px"}
The way we interact with audio production tools relies on the conventions established in the 1970s for audio engineers. 
Users communicate their audio concepts to these complex tools using knobs and sliders that control low-level technical parameters. 
Musicians currently need technical knowledge of signals in addition to their musical knowledge to make novel music. 
However, many experienced and casual musicians simply do not have the time or desire to acquire this technical knowledge. 
While simpler tools (e.g. Apple&apos;s GarageBand) exist, they are limiting and frustrating to users. 
In this research, I focus on bridging the gap between the intentions of both amateur and professional musicians and the audio manipulation tools available through software. 
Rather than force nonintuitive interactions, or remove control altogether, we reframe the controls to work within the interaction paradigms identified by research done on how audio engineers and musicians communicate auditory concepts to each other: evaluative feedback, natural language, vocal imitation, and exploration.

### Demos
- [SynthAssist](/publications/2014-07-01-cartwright2014synthassistquerying)
- [Mixploration](/publications/2014-04-01-cartwright2014mixploration)

### Selected publications ([see all](/publications/natural-audio-production-interfaces))
- [Learning to Build Natural Audio Production Interfaces (*Arts*)](/publications/2019-08-01-pardo2019learning)
- [VocalSketch: Vocally Imitating Audio Concepts (*CHI*)](/publications/2015-05-01-cartwright2015vocalsketch)
- [SynthAssist: Querying an Audio Synthesizer by Vocal Imitation (*NIME*)](/publications/2014-07-01-cartwright2014synthassistquerying)
- [Mixploration: Rethinking the Audio Mixer Interface (*IUI*)](/publications/2014-04-01-cartwright2014mixploration)
- [Social-EQ: Crowdsourcing an Equalization Descriptor Map (*ISMIR*)](/publications/2013-11-01-cartwright2013socialeq)

## Crowdsourced Audio Annotation and Quality Evaluation
![caqe](/assets/images/caqe.png){: .align-left height="250px" width="250px"}
In the past decade, audio researchers have often turned to crowdsourcing in order to hasten or scale their efforts for audio annotation and audio quality evaluation. 
However, the implications of this have been historically understudied. 
To address this, I research best practices for performing crowdsourced audio annotation and quality evaluation in order to obtain high quality data with high throughput. 
This includes investigating how data should be presented to annotators, how it should be aggregated (if necessary), and the expected annotation quality and througput.

### Selected publications ([see all](/publications/crowdsourced-audio-annotation-and-quality-evaluation))
- [Crowdsourcing Multi-label Audio Annotation Tasks with Citizen Scientists (*CHI*)](/publications/2019-04-01-cartwright2019crowdsourcing)
- [Crowdsourced Pairwise-Comparison for Source Separation Evaluation (*ICASSP*)](/publications/2018-04-01-cartwright2018crowdsourcedpairwise)
- [Investigating the Effect of Sound-Event Loudness on Crowdsourced Audio Annotations (*ICASSP*)](/publications/2018-04-01-cartwright2018investigating)
- [Seeing Sound: Investigating the Effects of Visualizations and Complexity on Crowdsourced Audio Annotations (*CSCW*)](/publications/2017-11-01-cartwright2017seeing)
- [Fast and Easy Crowdsourced Perceptual Audio Evaluation (*ICASSP*)](/publications/2016-04-01-cartwright2016fast)